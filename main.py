#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»ç¨‹åºï¼šAIæ–‡æ¡£ç”Ÿæˆå™¨
æ”¯æŒä¸‰é˜¶æ®µæµç¨‹ï¼šDOCè½¬æ¢ â†’ æ¨¡æ¿åˆ†æ â†’ JSONè¾“å…¥ â†’ æ–‡æ¡£ç”Ÿæˆ
"""

import os
import json
import logging
import subprocess
from datetime import datetime
from typing import Dict, Any, List, Optional
from docx import Document
from docx.shared import Inches, Pt
from openai import OpenAI
import base64
import mimetypes
import fitz  # PyMuPDF
from docx import Document as DocxDocument

# Load environment variables from .env file if it exists
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenv not installed, skip .env file loading
    pass

# Import prompts
from prompt_utils import get_fill_data_prompt, get_multimodal_extraction_prompt

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Define a directory for uploads and temporary files
UPLOADS_DIR = "uploads"
if not os.path.exists(UPLOADS_DIR):
    os.makedirs(UPLOADS_DIR)

class AIDocGenerator:
    """AIæ–‡æ¡£ç”Ÿæˆå™¨ - æ”¯æŒDOCè½¬æ¢"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯"""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.model = "google/gemini-2.5-pro-preview"
        logger.info("ğŸ¤– AIç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _extract_json_from_response(self, response_content: str) -> str:
        """
        Extract JSON string from AI response content.
        Handles various formats like markdown code blocks, plain JSON, etc.
        """
        if not response_content or not response_content.strip():
            raise ValueError("AI response content is empty")
        
        content = response_content.strip()
        
        # Try to extract from markdown code block
        if "```json" in content:
            try:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    json_str = content[start:end].strip()
                    if json_str:
                        return json_str
            except Exception:
                pass
        
        # Try to extract from single backticks
        if content.startswith("`") and content.endswith("`"):
            json_str = content.strip("`").strip()
            if json_str:
                return json_str
        
        # Try to find JSON object boundaries
        start_idx = content.find("{")
        if start_idx != -1:
            # Find the matching closing brace
            brace_count = 0
            for i, char in enumerate(content[start_idx:], start_idx):
                if char == "{":
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        json_str = content[start_idx:i+1]
                        # Validate it's proper JSON
                        try:
                            json.loads(json_str)
                            return json_str
                        except json.JSONDecodeError:
                            continue
        
        # If all else fails, try the content as-is
        try:
            json.loads(content)
            return content
        except json.JSONDecodeError:
            raise ValueError(f"Could not extract valid JSON from AI response: {content[:200]}...")

    def convert_doc_to_docx(self, doc_path: str) -> str:
        """
        ä½¿ç”¨LibreOfficeå°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ–‡ä»¶
        
        Args:
            doc_path: .docæ–‡ä»¶è·¯å¾„
            
        Returns:
            è½¬æ¢åçš„.docxæ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸ”„ å¼€å§‹DOCåˆ°DOCXè½¬æ¢...")
        
        if not os.path.exists(doc_path):
            logger.error(f"âŒ DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
            raise FileNotFoundError(f"DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        docx_path = doc_path.replace('.doc', '_converted.docx')
        
        try:
            # æ£€æŸ¥LibreOfficeæ˜¯å¦å¯ç”¨
            logger.info("ğŸ” æ£€æŸ¥LibreOfficeå¯ç”¨æ€§...")
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„LibreOfficeè·¯å¾„
            libreoffice_paths = [
                '/Applications/LibreOffice.app/Contents/MacOS/soffice',  # macOS
                'libreoffice',  # Linux/Windows PATH
                'soffice',  # å¤‡ç”¨å‘½ä»¤
            ]
            
            libreoffice_cmd = None
            for path in libreoffice_paths:
                try:
                    result = subprocess.run([path, '--version'], 
                                          capture_output=True, 
                                          text=True, 
                                          timeout=10)
                    if result.returncode == 0:
                        libreoffice_cmd = path
                        logger.info(f"âœ… æ‰¾åˆ°LibreOffice: {path}")
                        break
                except (FileNotFoundError, subprocess.TimeoutExpired):
                    continue
            
            if not libreoffice_cmd:
                logger.error("âŒ æœªæ‰¾åˆ°LibreOfficeï¼Œè¯·ç¡®ä¿å·²å®‰è£…LibreOffice")
                raise RuntimeError("LibreOfficeæœªå®‰è£…æˆ–ä¸å¯ç”¨")
            
            # æ‰§è¡Œè½¬æ¢
            logger.info(f"ğŸ“„ æ­£åœ¨è½¬æ¢: {doc_path} -> {docx_path}")
            
            # åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
            if os.path.exists(docx_path):
                os.remove(docx_path)
                logger.info("ğŸ—‘ï¸ åˆ é™¤å·²å­˜åœ¨çš„è½¬æ¢æ–‡ä»¶")
            
            # LibreOfficeè½¬æ¢å‘½ä»¤
            cmd = [
                libreoffice_cmd,
                '--headless',
                '--convert-to', 'docx',
                '--outdir', os.path.dirname(doc_path),
                doc_path
            ]
            
            logger.info(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=30)
            
            if result.returncode != 0:
                logger.error(f"âŒ LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                raise RuntimeError(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
            
            # æ£€æŸ¥è½¬æ¢åçš„æ–‡ä»¶
            expected_docx = doc_path.replace('.doc', '.docx')
            if os.path.exists(expected_docx):
                # é‡å‘½åä¸ºæˆ‘ä»¬æœŸæœ›çš„æ–‡ä»¶å
                if expected_docx != docx_path:
                    os.rename(expected_docx, docx_path)
                
                logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {docx_path}")
                return docx_path
            else:
                logger.error(f"âŒ è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°: {expected_docx}")
                raise RuntimeError("è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°")
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ LibreOfficeè½¬æ¢è¶…æ—¶")
            raise RuntimeError("LibreOfficeè½¬æ¢è¶…æ—¶")
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def stage1_analyze_template(self, template_path: str) -> Dict[str, str]:
        """
        é˜¶æ®µ1ï¼šç¡®å®šæ€§åœ°åˆ†æWordæ¨¡æ¿ï¼Œæå–å¸¦æœ‰ä½ç½®ä¿¡æ¯çš„ç»“æ„ã€‚
        
        Args:
            template_path: .docxæ¨¡æ¿æ–‡ä»¶è·¯å¾„

        Returns:
            ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯å•å…ƒæ ¼çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå€¼æ˜¯å•å…ƒæ ¼çš„æ–‡æœ¬å†…å®¹ã€‚
        """
        logger.info("ğŸ” é˜¶æ®µ1ï¼šå¼€å§‹ç¡®å®šæ€§æ¨¡æ¿ç»“æ„åˆ†æ...")
        
        try:
            doc = Document(template_path)
            template_structure = {}
            
            logger.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ¨¡æ¿æ–‡ä»¶: {template_path}")
            
            for i, table in enumerate(doc.tables):
                for j, row in enumerate(table.rows):
                    for k, cell in enumerate(row.cells):
                        cell_key = f"table_{i}_row_{j}_col_{k}"
                        template_structure[cell_key] = cell.text.strip()
            
            logger.info(f"âœ… æˆåŠŸæå– {len(template_structure)} ä¸ªå•å…ƒæ ¼çš„ç»“æ„ä¿¡æ¯ã€‚")
            # Log a snippet of the extracted structure for verification
            structure_snippet = json.dumps(dict(list(template_structure.items())[:5]), ensure_ascii=False, indent=2)
            logger.info(f"  ç»“æ„å®ä¾‹:\n{structure_snippet}")

            return template_structure
            
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ1é”™è¯¯: {e}")
            raise

    def stage2_load_json_data(self, json_file_path: str) -> Dict[str, Any]:
        """
        é˜¶æ®µ2ï¼šä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
        """
        logger.info("ğŸ“‚ é˜¶æ®µ2ï¼šå¼€å§‹åŠ è½½JSONæ•°æ®...")
        
        try:
            if not os.path.exists(json_file_path):
                logger.error(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
                raise FileNotFoundError(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
            
            logger.info(f"ğŸ“„ æ­£åœ¨è¯»å–JSONæ–‡ä»¶: {json_file_path}")
            
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ•°æ®å­—æ®µã€‚")
            for key, value in data.items():
                preview = str(value)[:70] + "..." if len(str(value)) > 70 else str(value)
                logger.info(f"   ğŸ“Œ {key}: {preview}")
            
            return data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ é˜¶æ®µ2é”™è¯¯: JSONæ–‡ä»¶æ ¼å¼æ— æ•ˆ - {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ2é”™è¯¯: {e}")
            raise
    
    def stage2_1_ai_extract_data_from_sources(self, attachment_paths: List[str]) -> Dict[str, Any]:
        """
        Stage 2.1: Use multimodal AI to extract data from various documents and images.
        """
        logger.info("ğŸ§  Stage 2.1: Kicking off multimodal AI data extraction...")
        
        try:
            # This is a sample schema. In a real app, this might come from the template
            # or a user configuration. For now, we'll use a schema based on sample_input.json
            fields_to_extract = json.dumps({
                "serial_number": "ç¤ºä¾‹: GZ-FH-2025-001",
                "project_name": "ç¤ºä¾‹: å†å²å»ºç­‘ä¿®å¤é¡¹ç›®",
                "review_date": "ç¤ºä¾‹: 2025-01-25",
                "original_condition_review": "å»ºç­‘ç‰©åŸå§‹çŠ¶æ€çš„æè¿°ã€‚",
                "damage_assessment_review": "å‘ç°çš„ä»»ä½•æŸä¼¤çš„è¯¦ç»†è¯„ä¼°ã€‚",
                "repair_plan_review": "æ‹Ÿå®šçš„ä¿®å¤è®¡åˆ’ã€‚",
                "project_lead": "é¡¹ç›®è´Ÿè´£äººå§“åã€‚",
                "reviewer": "å®¡æ ¸äººå‘˜å§“åã€‚",
                "damage_photos_path": "æŸä¼¤ç…§ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœæœ‰çš„è¯ã€‚",
                "site_photos_path": "ç°åœºç…§ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœæœ‰çš„è¯ã€‚",
                "attachments": "ç›¸å…³å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœæœ‰çš„è¯ã€‚ä¸ºæ¯ä¸ªå›¾åƒæä¾›æè¿°æ€§æ ‡é¢˜ã€‚"
            }, indent=2, ensure_ascii=False)

            prompt = get_multimodal_extraction_prompt(fields_to_extract)

            # Build the message with text and images
            content_parts = [{"type": "text", "text": prompt}]
            
            # --- Unified File Processing Loop ---
            image_paths_for_prompt = []
            temp_text_files = []

            for file_path in attachment_paths:
                file_name = os.path.basename(file_path)
                logger.info(f"ğŸ“„ Processing attachment: {file_name}")

                try:
                    if file_path.endswith(('.txt', '.md', '.json')):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            file_content = f.read()
                        text_part = f"\n\n--- Content from {file_name} ---\n{file_content}\n--- End of Content ---"
                        content_parts[0]["text"] += text_part

                    elif file_path.endswith('.docx'):
                        doc = DocxDocument(file_path)
                        full_text = "\n".join([p.text for p in doc.paragraphs])
                        text_part = f"\n\n--- Content from {file_name} ---\n{full_text}\n--- End of Content ---"
                        content_parts[0]["text"] += text_part

                    elif file_path.endswith('.pdf'):
                        doc = fitz.open(file_path)
                        full_text = ""
                        for page_num, page in enumerate(doc):
                            full_text += page.get_text()
                            # Extract images from PDF
                            img_list = page.get_images(full=True)
                            for img_index, img in enumerate(img_list):
                                xref = img[0]
                                base_image = doc.extract_image(xref)
                                image_bytes = base_image["image"]
                                image_ext = base_image["ext"]
                                
                                # Save image to a temporary file
                                temp_image_filename = f"pdf_{os.path.splitext(file_name)[0]}_p{page_num+1}_img{img_index}.{image_ext}"
                                temp_image_path = os.path.join(UPLOADS_DIR, temp_image_filename)
                                with open(temp_image_path, "wb") as f:
                                    f.write(image_bytes)
                                
                                image_paths_for_prompt.append(temp_image_path)
                                logger.info(f"ğŸ–¼ï¸  Extracted image from PDF: {temp_image_path}")
                        
                        text_part = f"\n\n--- Content from {file_name} ---\n{full_text}\n--- End of Content ---"
                        content_parts[0]["text"] += text_part
                        doc.close()

                    else: # Assumes it's an image if not a text-based file
                        mime_type, _ = mimetypes.guess_type(file_path)
                        if mime_type and mime_type.startswith('image/'):
                            image_paths_for_prompt.append(file_path)
                        else:
                            logger.warning(f"âš ï¸ Unsupported file type, skipping: {file_name}")

                except Exception as e:
                    logger.error(f"âŒ Error processing file {file_path}: {e}", exc_info=True)


            # Add all collected images to the prompt
            for image_path in image_paths_for_prompt:
                try:
                    mime_type, _ = mimetypes.guess_type(image_path)
                    with open(image_path, "rb") as image_file:
                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    
                    image_url = f"data:{mime_type};base64,{base64_image}"
                    
                    # Add a reference in the text part with Chinese description
                    content_parts[0]["text"] += f"\n\n--- é™„åŠ å›¾åƒ (æ–‡ä»¶è·¯å¾„: {image_path}) ---"
                    
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {"url": image_url}
                    })
                    logger.info(f"ğŸ–¼ï¸  Added image {image_path} to AI prompt.")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not process image file {image_path}: {e}")

            logger.info("ğŸ§  Calling multimodal AI to extract structured data... (This may take a moment)")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": content_parts}],
                temperature=0.1
            )
            
            # Clean up extracted text files
            for path in temp_text_files:
                try:
                    os.remove(path)
                except OSError as e:
                    logger.error(f"Error removing temp text file {path}: {e}")
            
            # Extract and parse the JSON from the AI's response
            if response.choices[0].message.content:
                json_string = self._extract_json_from_response(response.choices[0].message.content)
                extracted_data = json.loads(json_string)
                
                logger.info(f"âœ… AI successfully extracted data. Keys: {list(extracted_data.keys())}")
                return extracted_data
            else:
                raise ValueError("AI returned an empty response.")
                
        except Exception as e:
            logger.error(f"âŒ Stage 2.1 Error: {e}", exc_info=True)
            raise

    def stage2_5_ai_generate_fill_data(self, structured_template: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, str]:
        """
        é˜¶æ®µ2.5ï¼šä½¿ç”¨AIå°†è¾“å…¥æ•°æ®æ™ºèƒ½æ˜ å°„åˆ°æ¨¡æ¿ç»“æ„ï¼Œç”Ÿæˆç”¨äºå¡«å……çš„æœ€ç»ˆæ•°æ®ã€‚
        
        Args:
            structured_template: ä»é˜¶æ®µ1è·å¾—çš„æ¨¡æ¿ç»“æ„
            input_data: ä»é˜¶æ®µ2è·å¾—çš„è¾“å…¥æ•°æ®
            
        Returns:
            ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å•å…ƒæ ¼çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå€¼æ˜¯å¾…å¡«å……çš„æ•°æ®ã€‚
        """
        logger.info("ğŸ§  é˜¶æ®µ2.5ï¼šå¼€å§‹AIå­—æ®µæ˜ å°„å’Œæ•°æ®ç”Ÿæˆ...")
        
        try:
            # æ„å»ºAIæ˜ å°„æç¤º
            prompt = get_fill_data_prompt(
                json.dumps(structured_template, ensure_ascii=False, indent=2),
                json.dumps(input_data, ensure_ascii=False, indent=2)
            )
            
            logger.info("ğŸ§  æ­£åœ¨è°ƒç”¨AIç”Ÿæˆå¡«å……æ•°æ®... (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                extra_headers={
                    "HTTP-Referer": "ai-doc-generator",
                    "X-Title": "AI Document Generator",
                },
                temperature=0.1, # Use low temperature for more predictable output
            )

            # Log the full response for debugging purposes
            logger.info(f"ğŸ” Raw AI Response (for debugging):\n{response.model_dump_json(indent=2)}")

            # Gracefully handle invalid or empty responses from the API
            if not response or not response.choices:
                logger.error("âŒ AIå“åº”æ— æ•ˆæˆ–ä¸åŒ…å« 'choices' å­—æ®µã€‚")
                logger.error(f"   Full API Response: {response}")
                logger.warning("âš ï¸ AIå­—æ®µæ˜ å°„å¤±è´¥ï¼Œå°†è¿”å›ç©ºæ•°æ®ã€‚")
                return {}

            message = response.choices[0].message
            if not message or not message.content:
                logger.error("âŒ AIå“åº”çš„æ¶ˆæ¯å†…å®¹ä¸ºç©ºã€‚")
                logger.error(f"   Full choice object: {response.choices[0].model_dump_json(indent=2)}")
                logger.warning("âš ï¸ AIå­—æ®µæ˜ å°„å¤±è´¥ï¼Œå°†è¿”å›ç©ºæ•°æ®ã€‚")
                return {}
            
            # è§£æè¿”å›çš„JSON
            json_text = message.content
            if "```json" in json_text:
                json_text = json_text.split("```json")[1].split("```")[0]
            elif json_text.startswith("`") and json_text.endswith("`"):
                json_text = json_text.strip("`")

            fill_data = json.loads(json_text.strip())
            
            # Check for attachments in the AI response
            if '__attachments__' in fill_data:
                logger.info(f"ğŸ¯ AIç”Ÿæˆäº† {len(fill_data['__attachments__'])} ä¸ªé™„ä»¶å¼•ç”¨")
                for i, att in enumerate(fill_data['__attachments__']):
                    logger.info(f"   ğŸ“ é™„ä»¶ {i+1}: {att}")
            else:
                logger.info("â„¹ï¸ AIå“åº”ä¸­æœªåŒ…å«é™„ä»¶æ•°æ®")
            
            logger.info(f"âœ… AIæˆåŠŸç”Ÿæˆ {len(fill_data)} ä¸ªå­—æ®µçš„æ˜ å°„:")
            for key, value in fill_data.items():
                if key == '__attachments__':
                    logger.info(f"   ğŸ”— {key} -> [åŒ…å« {len(value)} ä¸ªé™„ä»¶]")
                else:
                    preview = str(value)[:70] + "..." if len(str(value)) > 70 else str(value)
                    logger.info(f"   ğŸ”— {key} -> '{preview}'")
            
            return fill_data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ é˜¶æ®µ2.5é”™è¯¯: AIè¿”å›çš„JSONæ— æ•ˆ - {e}")
            logger.error(f"   Raw AI Response: {json_text}")
            logger.warning("âš ï¸ AIå­—æ®µæ˜ å°„å¤±è´¥ï¼Œå°†è¿”å›ç©ºæ•°æ®ã€‚")
            return {}
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ2.5é”™è¯¯: {e}")
            logger.warning("âš ï¸ AIå­—æ®µæ˜ å°„å¤±è´¥ï¼Œå°†è¿”å›ç©ºæ•°æ®ã€‚")
            return {}

    def stage3_fill_template(self, template_path: str, output_path: str, fill_data: Dict[str, str]):
        """
        é˜¶æ®µ3ï¼šæ ¹æ®AIç”Ÿæˆçš„å¡«å……æ•°æ®ï¼Œç¡®å®šæ€§åœ°å¡«å……æ¨¡æ¿ã€‚
        
        Args:
            template_path: .docxæ¨¡æ¿æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            fill_data: ä»é˜¶æ®µ2.5è·å¾—çš„å¡«å……æ•°æ®
        """
        logger.info("ğŸ“ é˜¶æ®µ3ï¼šå¼€å§‹ç¡®å®šæ€§æ¨¡æ¿å¡«å……...")
        
        if not os.path.exists(template_path):
            logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
            raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")

        try:
            doc = Document(template_path)
            filled_fields_count = 0
            
            # Extract attachments before processing other fields
            attachments_data = fill_data.pop('__attachments__', [])
            logger.info(f"ğŸ“ å‘ç° {len(attachments_data)} ä¸ªé™„ä»¶å¾…å¤„ç†")
            if attachments_data:
                for i, att in enumerate(attachments_data):
                    logger.info(f"   é™„ä»¶ {i+1}: {att.get('title', 'N/A')} -> {att.get('path', 'N/A')}")
            
            # åˆ›å»ºä¸€ä»½å¾…å¡«å……å­—æ®µçš„å‰¯æœ¬ï¼Œç”¨äºè¿½è¸ª
            remaining_to_fill = set(fill_data.keys())

            for i, table in enumerate(doc.tables):
                for j, row in enumerate(table.rows):
                    for k, cell in enumerate(row.cells):
                        cell_key = f"table_{i}_row_{j}_col_{k}"
                        if cell_key in fill_data:
                            fill_value = str(fill_data[cell_key])
                            # æ¸…ç©ºå•å…ƒæ ¼åŸæœ‰å†…å®¹ï¼ˆå¦‚å ä½ç¬¦ï¼‰ï¼Œç„¶åå¡«å……
                            cell.text = fill_value
                            logger.info(f"   âœï¸ å¡«å…… {cell_key}: '{fill_value[:50]}...'")
                            filled_fields_count += 1
                            remaining_to_fill.discard(cell_key)

            # Add attachments at the end of the document
            if attachments_data:
                logger.info(f"ğŸ“ å¼€å§‹é™„åŠ  {len(attachments_data)} ä¸ªæ–‡ä»¶åˆ°æ–‡æ¡£æœ«å°¾...")
                # Add a page break before attachments if document is not empty
                if len(doc.paragraphs) > 0 or len(doc.tables) > 0:
                    doc.add_page_break()
                
                # Add a main heading for attachments section  
                # Use paragraph instead of heading to avoid style issues
                paragraph = doc.add_paragraph()
                run = paragraph.add_run("é™„ä»¶")
                run.bold = True
                run.font.size = Pt(16)  # Larger font size like a heading
                    
                for i, attachment in enumerate(attachments_data, 1):
                    title = attachment.get('title', f'é™„ä»¶ {i}')
                    path = attachment.get('path')
                    
                    if path and os.path.exists(path):
                        try:
                            # Add numbered attachment heading using paragraph  
                            heading_para = doc.add_paragraph()
                            heading_run = heading_para.add_run(f"{i}. {title}")
                            heading_run.bold = True
                            heading_run.font.size = Pt(14)  # Slightly smaller than main heading
                            
                            # Determine optimal image size based on file size and type
                            mime_type, _ = mimetypes.guess_type(path)
                            if mime_type and mime_type.startswith('image/'):
                                # Add the image with reasonable sizing
                                doc.add_picture(path, width=Inches(6.0))
                                logger.info(f"   âœ… å·²é™„åŠ å›¾ç‰‡: {path}")
                            else:
                                # For non-image files, add a note
                                p = doc.add_paragraph(f"æ–‡ä»¶: {os.path.basename(path)}")
                                logger.info(f"   ğŸ“„ å·²æ·»åŠ æ–‡ä»¶å¼•ç”¨: {path}")
                                
                        except Exception as e:
                            logger.error(f"   âŒ é™„åŠ æ–‡ä»¶å¤±è´¥ {path}: {e}")
                            # Add error note in document
                            doc.add_paragraph(f"âš ï¸ æ— æ³•æ˜¾ç¤ºé™„ä»¶: {os.path.basename(path) if path else 'Unknown'}")
                    else:
                        logger.warning(f"   âš ï¸ é™„ä»¶æ–‡ä»¶æœªæ‰¾åˆ°æˆ–è·¯å¾„æ— æ•ˆ: {path}")
                        # Add missing file note in document
                        doc.add_paragraph(f"âš ï¸ é™„ä»¶æ–‡ä»¶æœªæ‰¾åˆ°: {os.path.basename(path) if path else 'Unknown'}")

            # ä¿å­˜æ–‡æ¡£
            doc.save(output_path)
            
            logger.info(f"âœ… æ–‡æ¡£å·²æˆåŠŸç”Ÿæˆ: {output_path}")
            logger.info(f"ğŸ“Š å…±å¡«å…… {filled_fields_count} / {len(fill_data)} ä¸ªAIæ˜ å°„çš„å­—æ®µã€‚")

            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ˜ å°„çš„å­—æ®µæœªè¢«å¡«å……
            if remaining_to_fill:
                logger.warning("âš ï¸ ä»¥ä¸‹ç”±AIæ˜ å°„çš„å­—æ®µåœ¨æ¨¡æ¿ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„å•å…ƒæ ¼å¹¶è¢«è·³è¿‡ï¼š")
                for key in remaining_to_fill:
                    logger.warning(f"   - {key}")
            else:
                logger.info("âœ… æ‰€æœ‰AIæ˜ å°„çš„å­—æ®µéƒ½å·²æˆåŠŸå¡«å……ã€‚")
            
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ3é”™è¯¯: {e}")
            raise

    def run_generation(
        self, 
        doc_template_path: str, 
        output_path: str, 
        attachment_paths: Optional[List[str]] = None,
        direct_json_data: Optional[Dict[str, Any]] = None
    ):
        """
        Runs the full document generation process.

        Supports two workflows:
        1. If 'direct_json_data' is provided, it uses that data directly.
        2. If 'direct_json_data' is None, it runs AI extraction on 'attachment_paths'.
        """
        logger.info("ğŸš€ Starting unified document generation process...")
        
        try:
            # Stage 0: Convert .doc to .docx if necessary
            if doc_template_path.lower().endswith('.doc'):
                logger.info(f"ğŸ“„ Detected .doc template. Attempting conversion for: {doc_template_path}")
                processed_template_path = self.convert_doc_to_docx(doc_template_path)
            else:
                processed_template_path = doc_template_path

            # Stage 1 is always required to know the template structure
            template_structure = self.stage1_analyze_template(processed_template_path)
            
            input_data = {}
            if direct_json_data:
                logger.info("ğŸ“„ Using user-provided JSON data directly.")
                input_data = direct_json_data
            elif attachment_paths:
                logger.info("ğŸ§  No direct JSON provided, starting AI extraction from attachments.")
                # Stage 2.1: Use AI to extract data from attachments
                input_data = self.stage2_1_ai_extract_data_from_sources(
                    attachment_paths=attachment_paths
                )
            else:
                raise ValueError("Generation failed: You must provide either direct JSON data or attachment files.")

            # Stage 2.5: Use AI to map extracted/provided data to the template structure
            fill_data = self.stage2_5_ai_generate_fill_data(
                structured_template=template_structure,
                input_data=input_data
            )
            
            # Stage 3: Fill the Word template with the final data
            self.stage3_fill_template(
                template_path=processed_template_path,
                output_path=output_path,
                fill_data=fill_data
            )
            
            logger.info(f"âœ… Document generation complete: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Document generation failed: {e}", exc_info=True)
            return False

    def run_complete_workflow(self, doc_template_path: str, json_input_path: str, output_path: str):
        """
        è¿è¡Œå®Œæ•´çš„3é˜¶æ®µå·¥ä½œæµï¼ˆä»æ¨¡æ¿å’ŒJSONæ–‡ä»¶ï¼‰
        """
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„AIæ–‡æ¡£ç”Ÿæˆæµç¨‹")
        logger.info("=" * 60)
        
        start_time = datetime.now()

        # Create a dedicated directory for this generation job's intermediate files
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        job_dir = os.path.join("generated_docs", f"job_{timestamp}")
        os.makedirs(job_dir, exist_ok=True)
        logger.info(f"ğŸ“ Created job directory: {job_dir}")

        # åŸºäºè¾“å…¥æ¨¡æ¿åç§°ï¼Œåˆ›å»ºä¸­é—´æ–‡ä»¶çš„è·¯å¾„
        base_name = os.path.splitext(os.path.basename(doc_template_path))[0]
        structure_output_path = os.path.join(job_dir, f"{base_name}_template_structure.json")
        fill_data_output_path = os.path.join(job_dir, f"{base_name}_filled_data.json")
        
        try:
            # é˜¶æ®µ 0ï¼šDOCè½¬DOCX (å¦‚æœéœ€è¦)
            if doc_template_path.endswith('.doc'):
                docx_template_path = self.convert_doc_to_docx(doc_template_path)
            else:
                docx_template_path = doc_template_path
            logger.info("=" * 30)
            
            # é˜¶æ®µ 1ï¼šåˆ†ææ¨¡æ¿ç»“æ„
            structured_template = self.stage1_analyze_template(docx_template_path)
            
            # ä¿å­˜ä¸­é—´ç»“æœ1: æ¨¡æ¿ç»“æ„JSON
            try:
                with open(structure_output_path, 'w', encoding='utf-8') as f:
                    json.dump(structured_template, f, ensure_ascii=False, indent=4)
                logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {structure_output_path}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æ¨¡æ¿ç»“æ„JSONæ—¶å‡ºé”™: {e}")

            logger.info("=" * 30)
            
            # é˜¶æ®µ 2ï¼šåŠ è½½JSONæ•°æ®
            input_data = self.stage2_load_json_data(json_input_path)
            logger.info("=" * 30)
            
            # é˜¶æ®µ 2.5ï¼šAIç”Ÿæˆå¡«å……æ•°æ®
            fill_data = self.stage2_5_ai_generate_fill_data(structured_template, input_data)

            # ä¿å­˜ä¸­é—´ç»“æœ2: AIç”Ÿæˆçš„å¡«å……æ•°æ®JSON
            if fill_data:
                try:
                    with open(fill_data_output_path, 'w', encoding='utf-8') as f:
                        json.dump(fill_data, f, ensure_ascii=False, indent=4)
                    logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {fill_data_output_path}")
                except Exception as e:
                    logger.error(f"âŒ ä¿å­˜å¡«å……æ•°æ®JSONæ—¶å‡ºé”™: {e}")
            else:
                logger.warning("âš ï¸ AIæœªç”Ÿæˆå¡«å……æ•°æ®ï¼Œè·³è¿‡ä¿å­˜ä¸­é—´JSONæ–‡ä»¶ã€‚")
            
            logger.info("=" * 30)
            
            # é˜¶æ®µ 3ï¼šå¡«å……æ¨¡æ¿
            self.stage3_fill_template(docx_template_path, output_path, fill_data)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ AIæ–‡æ¡£ç”Ÿæˆæµç¨‹å®Œæˆ!")
            logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {duration:.2f} ç§’")
            logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_path}")
            if docx_template_path != doc_template_path:
                logger.info(f"ğŸ”„ ä¸­é—´è½¬æ¢æ–‡ä»¶: {docx_template_path}")
            logger.info(f"ğŸ“Š ä¸­é—´ç»“æ„æ–‡ä»¶: {structure_output_path}")
            if fill_data:
                logger.info(f"ğŸ§  ä¸­é—´å¡«å……æ•°æ®: {fill_data_output_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AIæ–‡æ¡£ç”Ÿæˆå™¨ - ä¸»ç¨‹åº")
    print("=" * 50)
    
    # --- é…ç½® ---
    # API Keyä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œç¡®ä¿å®‰å…¨æ€§
    # ä½ å¯ä»¥ä»è¿™é‡Œè·å–API Key: https://openrouter.ai/keys
    API_KEY = os.environ.get("OPENROUTER_API_KEY")
    
    if not API_KEY:
        logger.error("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENROUTER_API_KEY ç¯å¢ƒå˜é‡")
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        logger.error("  macOS/Linux: export OPENROUTER_API_KEY='your-api-key-here'")
        logger.error("  Windows: set OPENROUTER_API_KEY=your-api-key-here")
        logger.error("æˆ–è€…åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ : OPENROUTER_API_KEY=your-api-key-here")
        return

    # æ–‡ä»¶è·¯å¾„
    doc_template_path = "template_test2.doc"  # ä½¿ç”¨.docæˆ–.docxæ–‡ä»¶
    json_input_path = "sample_input2.json"
    output_path = f"AIç”Ÿæˆæ–‡æ¡£_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(doc_template_path):
        logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {doc_template_path}")
        return
    
    if not os.path.exists(json_input_path):
        logger.error(f"âŒ JSONè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {json_input_path}")
        return
    
    # åˆå§‹åŒ–å¹¶è¿è¡Œå·¥ä½œæµç¨‹
    try:
        generator = AIDocGenerator(API_KEY)
        success = generator.run_complete_workflow(
            doc_template_path=doc_template_path,
            json_input_path=json_input_path,
            output_path=output_path
        )
        
        if success:
            print(f"\nâœ… æˆåŠŸï¼ç”Ÿæˆçš„æ–‡æ¡£å·²ä¿å­˜è‡³: {output_path}")
        else:
            print("\nâŒ å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—ä¿¡æ¯ä»¥äº†è§£è¯¦æƒ…ã€‚")

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–æˆ–è¿è¡Œç”Ÿæˆå™¨æ—¶å‘ç”Ÿæœªå¤„ç†çš„é”™è¯¯: {e}", exc_info=True)


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦è¦å¯åŠ¨Webç•Œé¢
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--web":
        # å¯åŠ¨Webç•Œé¢
        import subprocess
        subprocess.run([sys.executable, "app.py"])
    elif len(sys.argv) > 1 and sys.argv[1] == "--cli":
        # å¯åŠ¨å‘½ä»¤è¡Œç•Œé¢
        main()
    else:
        # é»˜è®¤å¯åŠ¨Webç•Œé¢
        print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
        print("å¦‚éœ€ä½¿ç”¨å‘½ä»¤è¡Œç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ: python main.py --cli")
        print("=" * 50)
        import subprocess
        subprocess.run([sys.executable, "app.py"]) 